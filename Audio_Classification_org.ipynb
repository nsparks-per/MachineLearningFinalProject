{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoctFGvzXoPV"
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6Su2JiKL_1n"
   },
   "outputs": [],
   "source": [
    "#Installing the required packages\n",
    "!pip install librosa\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_MgrspRjV7m"
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy import fftpack\n",
    "from scipy.stats import kurtosis,skew,mode\n",
    "import sklearn.preprocessing,sklearn.decomposition\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold,train_test_split\n",
    "from sklearn.feature_selection import f_classif\n",
    "from keras import utils\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Dense, Dropout, Conv1D, Conv2D, Flatten,Reshape, BatchNormalization, ZeroPadding2D,MaxPooling1D,AveragePooling1D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers,optimizers\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lL6voiWiYEb4"
   },
   "source": [
    "**Extract Training Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8gjbD0TEL03"
   },
   "outputs": [],
   "source": [
    "#Function to extract the training data\n",
    "def get_training(original_path):\n",
    "    \n",
    "    # Load the training data csv file into a dataframe. \n",
    "    df = pd.read_csv(os.path.join(original_path,'train.csv'))\n",
    "\n",
    "    # Creating folder to store the Numpy arrays if they don't exist.\n",
    "    if not os.path.exists(os.path.join(original_path,'train_extracted')):\n",
    "        os.makedirs(os.path.join(original_path,'train_extracted'))\n",
    "\n",
    "    # Getting the file names of audios from the dataframe.\n",
    "    audio_files = np.array(df['new_id'])\n",
    "\n",
    "    # Load each audio file, save it as a numpy array\n",
    "    for i in range(len(audio_files)):    \n",
    "        file_name = str(audio_files[i])\n",
    "        d,r = librosa.load(os.path.join(\"Train\",file_name + \".wav\"),mono=True)\n",
    "        if (np.isnan(d).any()):\n",
    "            print(file_name)\n",
    "        np.save(os.path.join(original_path, 'train_extracted',file_name+'.npy'),d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI14BTKVYSLr"
   },
   "source": [
    "**Extract Testing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WScU_sGtcAfj"
   },
   "outputs": [],
   "source": [
    "#Function to extract the testing data\n",
    "def get_testing(original_path):\n",
    "\n",
    "    # Load the test data csv file into a dataframe.\n",
    "    df = pd.read_csv(os.path.join(original_path,'test_idx.csv'))\n",
    "\n",
    "    # Creating folder to store the Numpy arrays if they don't exist.\n",
    "    if not os.path.exists(os.path.join(original_path,'test_extracted')):\n",
    "        os.makedirs(os.path.join(original_path,'test_extracted'))\n",
    "\n",
    "    # Getting the file names of audios from the dataframe.\n",
    "    audio_files = np.array(df['new_id'])\n",
    "\n",
    "    # Load each audio file, save it as a numpy array\n",
    "    for i in range(len(audio_files)):   \n",
    "        file_name = str(audio_files[i])\n",
    "        d,r = librosa.load(os.path.join(\"Test\",file_name + \".wav\"),mono=True)\n",
    "        if (np.isnan(d).any()):\n",
    "            print(file_name)\n",
    "        np.save(os.path.join(original_path, 'test_extracted',file_name+'.npy'),d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tE8e35hgYnwy"
   },
   "source": [
    "**Extract MFCC Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCkWGUCNYpvY"
   },
   "outputs": [],
   "source": [
    "#Function to extract mfcc features which takes a csv file and the extracted folder as arguments\n",
    "def get_mfcc_features(original_path, csv_file, extracted_folder,num_coeffs):\n",
    "\n",
    "  # Load the csv file into a dataframe.\n",
    "  df = pd.read_csv(os.path.join(original_path, csv_file ))\n",
    "\n",
    "  # Get the audio file names.\n",
    "  audio_extracted = np.array(df['new_id'])\n",
    "\n",
    "  # Create an empty list to store the features.\n",
    "  mfcc_features=list()\n",
    "\n",
    "  # Looping on each Audio sequence array.\n",
    "  for i in range(len(audio_extracted)):\n",
    "        \n",
    "    # Load the Audio sequence.\n",
    "    audio_file_data = np.load(os.path.join(original_path, extracted_folder, str(audio_extracted[i])+'.npy'))\n",
    "\n",
    "    audio_file_data = librosa.effects.preemphasis(y=audio_file_data, coef=0.90)\n",
    "    # Calculate MFCC coefficients for the audio sequence.\n",
    "    mfcc_data = librosa.feature.mfcc(y=audio_file_data,sr=8000, n_mfcc = num_coeffs)\n",
    "\n",
    "    rms_data = librosa.feature.rms(y=audio_file_data, frame_length=128, hop_length=128)\n",
    "\n",
    "    rms_db = librosa.power_to_db(rms_data**2, ref=np.max)\n",
    "    # Calculating various statistic measures on the coefficients.\n",
    "\n",
    "    # print(np.mean(mfcc_data, axis=1))\n",
    "    # mfcc_data = mfcc_data - np.mean(mfcc_data, axis=1, keepdims=True)\n",
    "\n",
    "    # spec = librosa.feature.spectral_centroid(y=audio_file_data, sr=8000)\n",
    "\n",
    "    # spec_mean = np.mean(spec)\n",
    "\n",
    "    # print(spec_mean)\n",
    "\n",
    "    mean_mfcc = np.mean(mfcc_data, axis=1)\n",
    "\n",
    "    # mean_mfcc = np.mean(np.abs(np.diff(mfcc_data, axis=1)), axis=1)\n",
    "    # mean_mfcc= np.median(mfcc_data,axis=1)\n",
    "    # mean_mfcc = np.std(mfcc_data, axis=1)\n",
    "\n",
    "    addList = np.concatenate((np.array([np.mean(rms_db[0], axis=0)]), mean_mfcc))\n",
    "    mfcc_features.append(addList) \n",
    "\n",
    "    D = np.abs(librosa.stft(audio_file_data))\n",
    "\n",
    "    # Plot the spectrogram\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n",
    "    #                         y_axis='log', x_axis='time', sr=8000)\n",
    "    # plt.title('Spectrogram')\n",
    "    # plt.colorbar(format='%+2.0f dB')\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('STFT_0.png')\n",
    "    # plt.close\n",
    "\n",
    "    # break\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # librosa.display.specshow(mfcc_data, x_axis='time', sr=8000)\n",
    "    # plt.title('MFCCs')\n",
    "    # plt.colorbar()\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('MFCC_0.png')\n",
    "    # plt.close\n",
    "\n",
    "    # break\n",
    "    \n",
    "  # Return feature list.\n",
    "  return mfcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to use Support Vector Machines classifier\n",
    "def svm_classifier(X_train,Y_train,X_test, num_pcas):\n",
    "\n",
    "  # Intialize SVM classifier with One-vs-Rest decision function.\n",
    "  svm_model = svm.SVC(decision_function_shape='ovr')\n",
    "\n",
    "  # Fit the Training Dataset.\n",
    "  svm_model.fit(X_train, Y_train)\n",
    "\n",
    "  # Predict and return labels.\n",
    "  return svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract pca features which takes a csv file and the extracted folder as arguments\n",
    "def get_all_features(original_path, csv_file, extracted_folder):\n",
    "\n",
    "  # Load the csv file into a dataframe.\n",
    "  df = pd.read_csv(os.path.join(original_path, csv_file ))\n",
    "\n",
    "  # Get the audio file names.\n",
    "  audio_extracted = np.array(df['new_id'])\n",
    "\n",
    "  # Create an empty list to store the features.\n",
    "  all_features=list()\n",
    "\n",
    "  # Looping on each Audio sequence array.\n",
    "  for i in range(len(audio_extracted)):\n",
    "\n",
    "    # Load the Audio sequence.\n",
    "    audio_file_data= np.load(os.path.join(original_path, extracted_folder, str(audio_extracted[i])+'.npy'))\n",
    "\n",
    "    # Calculate MFCC on the audio sequence.\n",
    "    mfcc_data = librosa.feature.mfcc(y=audio_file_data,sr=22050)\n",
    "\n",
    "    # Calculate Root Mean Square Error.\n",
    "    rmse= librosa.feature.rms(y=audio_file_data)\n",
    "\n",
    "    # Calculate Chroma STFTs.\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=audio_file_data, sr=22050),axis=1)\n",
    "\n",
    "    # Calculate Spectral Centroid.\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=audio_file_data, sr=22050)\n",
    "\n",
    "    # Calculate Spectral Bandwidth.\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=audio_file_data, sr=22050)\n",
    "\n",
    "    # Calculate Rolloff.\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio_file_data, sr=22050)\n",
    "\n",
    "    # Calculate Zero Crossing Rate.\n",
    "    zcr =librosa.feature.zero_crossing_rate(audio_file_data)\n",
    "\n",
    "    # Calculate and append statistic features for all the above data features.\n",
    "    addList = np.concatenate((np.mean(mfcc_data, axis=1),np.median(mfcc_data,axis=1),np.std(mfcc_data, axis=1),skew(mfcc_data, axis=1),kurtosis(mfcc_data, axis=1),np.atleast_1d(np.mean(rmse)),np.atleast_1d(np.median(rmse)),np.atleast_1d(np.std(rmse)),np.atleast_1d(skew(rmse,axis=1)),np.atleast_1d(kurtosis(rmse,axis=1)),np.atleast_1d(np.mean(chroma_stft)),np.atleast_1d(np.median(chroma_stft)),np.atleast_1d(np.std(chroma_stft)),np.atleast_1d(skew(chroma_stft)),np.atleast_1d(kurtosis(chroma_stft)),np.atleast_1d(np.mean(spec_cent)),np.atleast_1d(np.median(spec_cent)),np.atleast_1d(np.std(spec_cent)),np.atleast_1d(skew(spec_cent,axis=1)),np.atleast_1d(kurtosis(spec_cent,axis=1)),np.atleast_1d(np.mean(spec_bw)),np.atleast_1d(np.median(spec_bw)),np.atleast_1d(np.std(spec_bw)),np.atleast_1d(skew(spec_bw,axis=1)),np.atleast_1d(kurtosis(spec_bw,axis=1)),np.atleast_1d(np.mean(rolloff)),np.atleast_1d(np.median(rolloff)),np.atleast_1d(np.std(rolloff)),np.atleast_1d(skew(rolloff,axis=1)),np.atleast_1d(kurtosis(rolloff,axis=1)),np.atleast_1d(np.mean(zcr)),np.atleast_1d(np.median(zcr)),np.atleast_1d(np.std(zcr)),np.atleast_1d(skew(zcr,axis=1)),np.atleast_1d(kurtosis(zcr,axis=1)),np.amax(mfcc_data, axis=1),np.amin(mfcc_data, axis=1)))\n",
    "    all_features.append(addList) \n",
    "    if (np.isnan(addList).any()):\n",
    "      print(audio_extracted[i])\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract the features of training and testing data\n",
    "def get_pca_features(original_path, train_csv, train_extracted,test_csv, test_extracted,no_of_components):\n",
    "\n",
    "  #no_of_components specify the required number of components\n",
    "  # Extract all the best features for the train and test audio sequences.\n",
    "  train_features=get_all_features(original_path, train_csv, train_extracted)\n",
    "  test_features =get_all_features(original_path, test_csv, test_extracted)\n",
    "\n",
    "  print(train_features)\n",
    "  \n",
    "  # Standardize the features.\n",
    "  sc = StandardScaler(with_mean=False)\n",
    "  train_features = sc.fit_transform(train_features)\n",
    "  test_features = sc.transform(test_features)\n",
    "\n",
    "  # Apply PCA with no of components satisfying the explained variance ratio.\n",
    "  pca = sklearn.decomposition.PCA()\n",
    "  pca.fit(train_features)\n",
    "  cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "  #no_of_components = np.argmax(cumsum >= 0.95) + 1\n",
    "  pca = sklearn.decomposition.PCA(n_components=no_of_components)\n",
    "\n",
    "  # Fit and Transform training and testing data.\n",
    "  train_features = pca.fit_transform(train_features)\n",
    "  test_features = pca.transform(test_features)\n",
    "    \n",
    "  return train_features,test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9BinYI7Zd8M"
   },
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43rMschNA5_U"
   },
   "outputs": [],
   "source": [
    "#Function to use Random Forest classifier\n",
    "def random_forest(X_train,Y_train,X_test, num_trees):\n",
    "\n",
    "  # Intialize Random Forest classifier with number of trees as 800.\n",
    "  random_forest = RandomForestClassifier(n_estimators= num_trees)\n",
    "\n",
    "  # Fit Training Dataset.\n",
    "  random_forest.fit(X_train, Y_train)\n",
    "\n",
    "  # Predict and return labels.\n",
    "  return random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xhxkr8v12GvA"
   },
   "outputs": [],
   "source": [
    "#Funtion to retrieve the labels of training data\n",
    "def get_labels(original_path,csv_file):\n",
    "\n",
    "  # Load the csv file into a dataframe.\n",
    "  df = pd.read_csv(os.path.join(original_path, csv_file ))\n",
    "\n",
    "  # Return the labels.\n",
    "  labels = np.array(df['genre'])\n",
    "  speakers = np.array(df['speaker'])\n",
    "\n",
    "  return labels, speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDb6si5d-MAu"
   },
   "outputs": [],
   "source": [
    "#Funtion to standardize the features\n",
    "def standardize_features(X_train,X_test):\n",
    "\n",
    "  # Initialize standard scalar with zero mean\n",
    "  sc = StandardScaler(with_mean=True)\n",
    "\n",
    "  # Fit and transform the Training Dataset.\n",
    "  X_train= sc.fit_transform(X_train)\n",
    "\n",
    "  # Transform the testing set.\n",
    "  X_test = sc.transform(X_test)\n",
    "  \n",
    "  return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6c5GkrPFVsvV"
   },
   "outputs": [],
   "source": [
    "#Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true,y_pred,label_names,classifier, num_ceoffs, num_trees):\n",
    "    \n",
    "    # Calculate the confusion matrix using the expected and predicted values.\n",
    "    confusion_mat = confusion_matrix(np.array(y_true),y_pred,labels=label_names)\n",
    "    \n",
    "    #  Show the confusion matrix values.\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.imshow(confusion_mat, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "    \n",
    "    # Set the x, y and title labels for the plot.\n",
    "    plt.xlabel(\"Expected Outputs\", fontsize=10)\n",
    "    plt.ylabel(\"Actual Outputs\", fontsize=10)\n",
    "    plt.title(\"Confusion Matrix of \"+ classifier + \" classifier\",fontsize=12)\n",
    "    \n",
    "    # Arrange the label names on the x and y axis.\n",
    "    plt.xticks(np.arange(len(label_names)), label_names, rotation='vertical')\n",
    "    plt.yticks(np.arange(len(label_names)), label_names)\n",
    "    plt.tick_params(axis='both', labelsize='10')\n",
    "    plt.tight_layout()\n",
    "    for (y, x), label in np.ndenumerate(confusion_mat):\n",
    "        if label != 0:\n",
    "            plt.text(x,y,label,ha='center',va='center', size='12')\n",
    "            \n",
    "    # Show the plot\n",
    "    plt.savefig('Images/confusion_matrix_coeffs' + str(num_ceoffs) + '_trees_' + str(num_trees) + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcrW-s_ADvAJ"
   },
   "outputs": [],
   "source": [
    "#Function to perform cross validation\n",
    "def cross_validate(X_train,Y_train,data_rep, num_ceoffs, num_trees):\n",
    "\n",
    " # Choose classifiers based on the Data Representation.\n",
    " if data_rep==\"mfcc\":\n",
    "    clf = random_forest\n",
    "    classifier=\"Random Forest\"\n",
    "    sc= StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    " elif data_rep==\"pca\":\n",
    "    clf = svm_classifier\n",
    "    classifier=\"SVM\"\n",
    "\n",
    " #Create the list with actual Label names.\n",
    " label_names=[\"Zero\",\"One\",\"Two\",\"Three\",\"Four\",\"Five\",\"Six\",\"Seven\",\"Eight\",\"Nine\"]\n",
    "\n",
    " # Create a Stratified KFold with 5 splits.\n",
    " k_fold = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "\n",
    " # Create two empty lists to keep track of accuracies for plotting and predictions for confusion matrix.\n",
    " accuracies_clf = list()\n",
    " predictions_clf = list()\n",
    " actual_predictions = list()\n",
    "\n",
    " # Split the Dataset and Loop through each fold.\n",
    " for train_index, test_index in k_fold.split(X_train,Y_train):\n",
    "\n",
    "    # Split the Dataset into Training and Validation.\n",
    "    x_train, x_test = X_train[train_index], X_train[test_index]\n",
    "    y_train, y_test = Y_train[train_index], Y_train[test_index]    \n",
    "\n",
    "    # Fit and Predict the current fold using the classifier.\n",
    "    pred = clf(x_train,y_train,x_test, num_trees)\n",
    "\n",
    "    # Append the accuracies and predictions.\n",
    "    predictions_clf.append(pred)\n",
    "    actual_predictions.append(y_test)\n",
    "    accuracies_clf.append(accuracy_score(pred,y_test))\n",
    " \n",
    " # Get the corresponding label names for the predictions.\n",
    " predictions_clf = np.array(predictions_clf).flatten()\n",
    " actual_predictions = np.array(actual_predictions).flatten()\n",
    "\n",
    " predictions_clf = [label_names[predictions_clf[i]] for i in range(len(predictions_clf))]\n",
    " actual_predictions = [label_names[actual_predictions[i]] for i in range(len(actual_predictions))]\n",
    "\n",
    " # Plot the confusion matrix for the classifier.\n",
    " plot_confusion_matrix(actual_predictions,predictions_clf,label_names,classifier,num_ceoffs,num_trees)\n",
    "\n",
    " # Plot the accuracy of the classifier for different folds\n",
    "#  plt.plot([1,2,3,4,5],accuracies_clf)\n",
    "#  plt.xticks(np.arange(1,5,1))\n",
    "#  plt.xlabel(\"5-Fold CV\")\n",
    "#  plt.ylabel(\"Accuracy\")\n",
    "#  plt.show() \n",
    "\n",
    " # Calculate and print the accuracy range with 99% confidence interval\n",
    " f =  f1_score(predictions_clf,actual_predictions, average='macro')\n",
    " accuracy_clf= accuracy_score(predictions_clf,actual_predictions)\n",
    " radius_clf = 2.58 * np.sqrt(accuracy_clf*(1-accuracy_clf)/len(predictions_clf))\n",
    " accuracy_clf_min =  (accuracy_clf - radius_clf)\n",
    " accuracy_clf_max =  (accuracy_clf + radius_clf)\n",
    "\n",
    " radius_clf = 2.58 * np.sqrt(f*(1-f)/len(predictions_clf))\n",
    " f_clf_min =  (f - radius_clf)\n",
    " f_clf_max =  (f + radius_clf)\n",
    "\n",
    " print('\\n\\n Accuracy of '+ classifier +' on Validation Dataset is ' + str(accuracy_clf))\n",
    " print('\\n At 99% Confidence Interval :')\n",
    " print('\\n The Accuracy of ' + classifier + ' is likely between ' + str(accuracy_clf_min) + ' and ' + str(accuracy_clf_max))\n",
    "\n",
    " print('\\n\\n Accuracy of '+ classifier +' on Validation Dataset is ' + str(f))\n",
    " print('\\n At 99% Confidence Interval :')\n",
    " print('\\n The Accuracy of ' + classifier + ' is likely between ' + str(f_clf_min) + ' and ' + str(f_clf_max))\n",
    "\n",
    " return(accuracy_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyjxXLmxaRRv"
   },
   "source": [
    "**Extract Training and Testing Audio files into Numpy Arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76cbJrV9fAOr"
   },
   "outputs": [],
   "source": [
    "# Set the path of the project folder which has train, test folders and train.csv, test_idx.csv\n",
    "original_path = os.path.realpath(\".\")\n",
    "\n",
    "# Extract train and test dataset into numpy arrays and save them\n",
    "get_training(original_path)\n",
    "get_testing(original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and test set for ANOVA\n",
    "num_coeffs = 12\n",
    "X_train = get_mfcc_features(original_path,'train.csv','train_extracted', num_coeffs)\n",
    "Y_train, speaker = get_labels(original_path,'train.csv')\n",
    "X_test = get_mfcc_features(original_path,'test_idx.csv','test_extracted', num_coeffs)\n",
    "X_train,X_test = standardize_features(X_train,X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify and display the results of the 4 best features.\n",
    "print(librosa.mel_frequencies(n_mels=num_coeffs, fmin=0, fmax=4000))\n",
    "\n",
    "color_dict = {'george':'red', 'jackson':'blue', 'lucas':'black', 'nicolas':'green', 'theo':'purple', 'yweweler':'orange'}\n",
    "\n",
    "print(np.array(X_train).shape)\n",
    "print(X_train[0])\n",
    "# X_train = np.array(X_train)\n",
    "# print(len(X_train[:,1]))\n",
    "\n",
    "# plt.scatter(X_train[:,0], X_train[:,1], c=Y_train)\n",
    "# plt.show()\n",
    "# plt.scatter(X_train[:,2], X_train[:,3], c=Y_train)\n",
    "# plt.show()\n",
    "\n",
    "f, p = f_classif(X_train, Y_train)\n",
    "\n",
    "most_var = np.flip(np.argsort(f))\n",
    "print(most_var)\n",
    "\n",
    "for k in list(range(0,4)):\n",
    "    plt.scatter(X_train[:,most_var[k]], Y_train, color=[color_dict[i] for i in speaker ])\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.xlabel(\"Feature Level\")\n",
    "    plt.title('Feature ' + str(k))\n",
    "    plt.show()\n",
    "\n",
    "print(f[most_var])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hb9DD8JVbKcm"
   },
   "source": [
    "**Random Forest on MFCC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9-uw37lzAmIw",
    "outputId": "af2a6e81-3db3-4a73-d0b8-f1bc4456d994"
   },
   "outputs": [],
   "source": [
    "# Conduct search on the number of MFCC coeffs\n",
    "accuracy = []\n",
    "f1_total = []\n",
    "trees = 20\n",
    "    \n",
    "for i in range(8,17):\n",
    "    # Extract MFCC features and standardize them.\n",
    "    X_train = get_mfcc_features(original_path,'train.csv','train_extracted', i)\n",
    "\n",
    "    Y_train, _ = get_labels(original_path,'train.csv')\n",
    "    X_test = get_mfcc_features(original_path,'test_idx.csv','test_extracted', i)\n",
    "    X_train,X_test = standardize_features(X_train,X_test)\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "    print(np.array(Y_train).shape)\n",
    "\n",
    "    # Cross Validate MFCC Features.\n",
    "\n",
    "    print(Y_train)\n",
    "    acc, f1 = (cross_validate(np.array(X_train),np.array(Y_train).flatten(),\"mfcc\", i, trees))\n",
    "    accuracy.append(acc)\n",
    "    f1_total.append(f1)\n",
    "\n",
    "    # Classify MFCC Test Features using Random Forest Classifier.\n",
    "    y_test_rf = random_forest(X_train,Y_train,X_test, trees)\n",
    "    Y_test = pd.read_csv(os.path.join(original_path,'test_idx.csv'))\n",
    "    Y_test['genre'] = y_test_rf.tolist()\n",
    "    Y_test = Y_test.rename(columns={\"new_id\":\"id\"})\n",
    "    Y_test.to_csv(os.path.join(original_path,'predict_rf.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 score and accuracy\n",
    "print(len(range(8,17)))\n",
    "print(len(accuracy))\n",
    "\n",
    "plt.plot(range(8,17),accuracy)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"# MFCC Coeff\")\n",
    "plt.savefig('Images\\Accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "print(len(range(8,17)))\n",
    "print(len(f1_total))\n",
    "\n",
    "plt.plot(range(8,17),f1_total)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"# MFCC Coeff\")\n",
    "plt.savefig('Images\\F1Total.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm = []\n",
    "f1_svm = []\n",
    "i = 4\n",
    "for i in range(2,14):\n",
    "    # Extract PCA features.\n",
    "    X_train,X_test = get_pca_features(original_path,'train.csv','train_extracted','test_idx.csv','test_extracted',i)\n",
    "    Y_train, _ = get_labels(original_path,'train.csv')\n",
    "\n",
    "    # Cross Validate PCA Features.\n",
    "    accuracy_svm_i, f1_svm_i = cross_validate(np.array(X_train),np.array(Y_train),\"pca\", i, 1)\n",
    "\n",
    "    accuracy_svm.append(accuracy_svm_i)\n",
    "    f1_svm.append(f1_svm_i)\n",
    "\n",
    "    # Classify PCA Test Features using SVM classifier.\n",
    "    y_test_svm = svm_classifier(X_train,Y_train,X_test, i)\n",
    "    Y_test = pd.read_csv(os.path.join(original_path,'test_idx.csv'))\n",
    "    Y_test['genre'] = y_test_svm.tolist()\n",
    "    Y_test = Y_test.rename(columns={\"new_id\":\"id\"})\n",
    "    Y_test.to_csv(os.path.join(original_path,'predict_svm.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 score and accuracy\n",
    "print(len(range(2,14)))\n",
    "print(len(accuracy_svm))\n",
    "\n",
    "plt.plot(range(2,14),accuracy_svm)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"# PCA Coeff\")\n",
    "plt.savefig('Images\\SVMAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "print(len(range(2,14)))\n",
    "print(len(f1_svm))\n",
    "\n",
    "plt.plot(range(2,14),f1_svm)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"# PCA Coeff\")\n",
    "plt.savefig('Images\\SVMF1Total.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AudioCategorization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
